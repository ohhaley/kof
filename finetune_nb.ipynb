{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed8daab2-0736-46ff-8839-e68185ace8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\['\n",
      "/tmp/ipykernel_47748/1511640490.py:70: SyntaxWarning: invalid escape sequence '\\['\n",
      "  data = pd.read_csv(file_name, header=None, names=column_names, sep='\\[\\[::\\]\\]', engine='python', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "\n",
    "\n",
    "def format_messages(entry):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': entry['prompt']},\n",
    "        {'role': 'assistant', 'content': entry['reasoning']}\n",
    "    ]\n",
    "    return pd.Series({'messages': messages, 'label': entry['response 2']})\n",
    "\n",
    "def validate_dataset(dataset, tokenizer):\n",
    "    print(\"--- Validating Dataset for Assistant Tokens ---\")\n",
    "    found_issue = False\n",
    "    \n",
    "    for i in range(min(5, len(dataset))):  # Check first 5 samples\n",
    "        sample = dataset[i]\n",
    "        # Simulate what the trainer does\n",
    "        formatted_text = tokenizer.apply_chat_template(sample['messages'], tokenize=False)\n",
    "        \n",
    "        # Check if 'assistant' header exists in the string\n",
    "        if \"<|im_start|>assistant\" not in formatted_text:\n",
    "            print(f\"Row {i} is missing assistant markers!\")\n",
    "            print(f\"Content: {formatted_text[:100]}...\")\n",
    "            found_issue = True\n",
    "            \n",
    "    if not found_issue:\n",
    "        print(\"Chat template looks good.\")\n",
    "\n",
    "\n",
    "# Read in the fine tuning csv\n",
    "# ---------- PREPROCESSING ----------\n",
    "def preprocess_data(file_name):\n",
    "\n",
    "    good_threshold = 2.3\n",
    "    evil_threshold = 1.0\n",
    "\n",
    "    thresholds = {\n",
    "        'WASHERWOMAN': good_threshold,\n",
    "        'LIBRARIAN': good_threshold,\n",
    "        'INVESTIGATOR': good_threshold,\n",
    "        'CHEF': good_threshold,\n",
    "        'EMPATH': good_threshold,\n",
    "        'FORTUNETELLER': good_threshold,\n",
    "        'UNDERTAKER': good_threshold,\n",
    "        'MONK': good_threshold,\n",
    "        'RAVENKEEPER': good_threshold,\n",
    "        'VIRGIN': good_threshold,\n",
    "        'SLAYER': good_threshold,\n",
    "        'SOLDIER': good_threshold,\n",
    "        'MAYOR': good_threshold,\n",
    "        'BUTLER': good_threshold,\n",
    "        'SAINT': good_threshold,\n",
    "        'RECLUSE': good_threshold,\n",
    "        'DRUNK': good_threshold,\n",
    "\n",
    "        'POISONER': evil_threshold,\n",
    "        'SPY': evil_threshold,\n",
    "        'BARON': evil_threshold,\n",
    "        'SCARLET WOMAN': evil_threshold,\n",
    "        'IMP': evil_threshold\n",
    "    }\n",
    "\n",
    "    evil_roles = ['POISONER','SPY','BARON','SCARLET WOMAN','IMP']\n",
    "\n",
    "    column_names = ['name', 'role', 'prompt', 'reasoning', 'response 2', 'label']\n",
    "    data = pd.read_csv(file_name, header=None, names=column_names, sep='\\[\\[::\\]\\]', engine='python', on_bad_lines='skip')\n",
    "\n",
    "    data = data.dropna(subset=['prompt', 'reasoning'])\n",
    "    data = data[data['reasoning'].str.strip() != \"\"]\n",
    "\n",
    "    # game_info is a prefix to remove from 'prompt 1'\n",
    "    with open(\"systemprompt.md\", 'r') as f: game_info = f.read()\n",
    "    data['prompt'] = data['prompt'].str.removeprefix(game_info)\n",
    "\n",
    "    val = pd.to_numeric(data['label'], errors='coerce')\n",
    "    thresh = data['role'].map(thresholds)\n",
    "\n",
    "    mask = (data['role'].isin(evil_roles) & (val > thresh)) | (~data['role'].isin(evil_roles) & (val < thresh))\n",
    "\n",
    "    filtered_data = data[mask]\n",
    "\n",
    "    # Convert response 2 to a dict from a string\n",
    "    # data['response 2'] = data['response 2'].apply(convert_to_dict)\n",
    "\n",
    "    # dataset = Dataset.from_pandas(data)\n",
    "    dataset = filtered_data.apply(format_messages, axis=1)\n",
    "    # tokenized_data = formatted_data.map(tokenize_func, batched=True)\n",
    "\n",
    "    # Shuffle and split data into two sets\n",
    "    # Do this last as well\n",
    "    d_dataset = Dataset.from_pandas(dataset)\n",
    "    dataset_split = d_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    train_dataset = dataset_split[\"train\"]\n",
    "    eval_dataset = dataset_split[\"test\"]\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def fine_tune():\n",
    "\n",
    "    td, ed = preprocess_data('game1.csv')\n",
    "    validate_dataset(td, tokenizer)\n",
    "\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=\"./qwen3-sft-output\",\n",
    "        max_length=2048,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        bf16=False, # Use bfloat16 for Qwen3 if your GPU supports it\n",
    "        logging_steps=1,\n",
    "        eval_strategy='epoch',\n",
    "        eval_steps=1,\n",
    "        do_eval=True,\n",
    "        assistant_only_loss=True,\n",
    "        packing=False\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=repo_id,\n",
    "        train_dataset=td,\n",
    "        eval_dataset=ed,\n",
    "        args=sft_config,\n",
    "        processing_class=tokenizer\n",
    "    )\n",
    "\n",
    "    # trainer is now trained.\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "# TODO\n",
    "# make sure to get data in and randomize, split, etc.\n",
    "# what formats of data can we even accept?\n",
    "# HOW DO WE MAKE IT CLEAR THE LABEL IS WHAT WE WANT TO BE PREDICTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e1760e-c175-40c9-9eb6-6a5c7ec61ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/zfs/2022/nkonarst/CSDS373/kof/myenv/lib/python3.12/site-packages\")\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c571342-2de1-4be1-a3aa-3e7fa0a216bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = 'Qwen/Qwen3-4B-Instruct-2507'\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{{ '<|im_start|>' + message['role'] + '\\\\n' }}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "        \"{% generation %}\"\n",
    "        \"{% if message.get('thinking') %}\"\n",
    "            \"{{ '<think>\\\\n' + message['thinking'] + '\\\\n</think>\\\\n' }}\"\n",
    "        \"{% endif %}\"\n",
    "        \"{{ message['content'] }}\"\n",
    "        \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "        \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{{ '<|im_end|>\\\\n' }}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '<|im_start|>assistant\\\\n' }}\"\n",
    "    \"{% endif %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a79d0043-e2b8-480a-a8a1-9509dd5bd4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validating Dataset for Assistant Tokens ---\n",
      "Chat template looks good.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9f92e43c374df4baefc4e600566bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28498a1e88247bea88503251aba56eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82570a3d7e34d02841bcd4c450011a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cbd5e3007748f69e1d455e6990dc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5941c4b7c93408fa9eaa44da5dd66fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47748/1511640490.py:70: SyntaxWarning: invalid escape sequence '\\['\n",
      "  data = pd.read_csv(file_name, header=None, names=column_names, sep='\\[\\[::\\]\\]', engine='python', on_bad_lines='skip')\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 11.72 GiB of which 66.19 MiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 11.49 GiB is allocated by PyTorch, and 1.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mfine_tune\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    105\u001b[39m validate_dataset(td, tokenizer)\n\u001b[32m    107\u001b[39m sft_config = SFTConfig(\n\u001b[32m    108\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./qwen3-sft-output\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m     max_length=\u001b[32m2048\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m     packing=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43med\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# trainer is now trained.\u001b[39;00m\n\u001b[32m    131\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zfs/2022/nkonarst/CSDS373/kof/myenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:844\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28mself\u001b[39m._total_train_tokens = \u001b[32m0\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;66;03m# Initialize the Trainer. Parent class will handle:\u001b[39;00m\n\u001b[32m    839\u001b[39m \u001b[38;5;66;03m# - DeepSpeed configuration (through create_accelerator_and_postprocess)\u001b[39;00m\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# - FSDP setup\u001b[39;00m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# - Distributed training setup\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# - Optimizer and scheduler creation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# Initialize activation offloading context\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.activation_offloading:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zfs/2022/nkonarst/CSDS373/kof/myenv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zfs/2022/nkonarst/CSDS373/kof/myenv/lib/python3.12/site-packages/transformers/trainer.py:612\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    609\u001b[39m     \u001b[38;5;28mself\u001b[39m.place_model_on_device\n\u001b[32m    610\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mquantization_method\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) != QuantizationMethod.BITS_AND_BYTES\n\u001b[32m    611\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_model_parallel:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zfs/2022/nkonarst/CSDS373/kof/myenv/lib/python3.12/site-packages/transformers/trainer.py:910\u001b[39m, in \u001b[36mTrainer._move_model_to_device\u001b[39m\u001b[34m(self, model, device)\u001b[39m\n\u001b[32m    906\u001b[39m     logger.warning(\n\u001b[32m    907\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe model is already on multiple devices. Skipping the move to device specified in `args`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    908\u001b[39m     )\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m910\u001b[39m model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.parallel_mode == ParallelMode.TPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mtie_weights\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zfs/2022/nkonarst/CSDS373/kof/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/jupyter/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/jupyter/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/jupyter/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/jupyter/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/jupyter/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 11.72 GiB of which 66.19 MiB is free. Including non-PyTorch memory, this process has 11.64 GiB memory in use. Of the allocated memory 11.49 GiB is allocated by PyTorch, and 1.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "fine_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9831aa-3bda-4b3d-b325-359b8c704d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
